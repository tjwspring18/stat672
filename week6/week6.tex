\documentclass{minimal}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}

\textbf{Linear classifiers}

Linear discriminant analysis (LDA)

Logistic regression

Support vector machine (SVM)

\medskip

\textbf{LDA}

Assume classes are multivariate Gaussian with common covariance matrix $\mathbf{\Sigma}$

It can be shown that under these assumptions the decision boundary between any
two classes is linear, i.e. a hyperplane in $\mathbb{R}$.

\smallskip

$w$ is a weight vector that defines a projection into a 1-dimensional subspace.
Then, in that sub-space, we set a threshold $t$ as the boundary between the two
classes. 

\includegraphics[scale=0.25]{lda}

Fisher's optimal projection: maximize between-class variance relative
to within-class variance. In other words: projected class centroids are far
apart, projected data are close to centroids.

$$
\hat{w} = \argmax_{w \in \mathbb{R}^d} \frac{w'\mathbf{B}w}{w'\mathbf{W}w}
$$

$$
\mathbf{B} = \sum_c (\bm{\mu}_c - \bar{\mathbf{x}}) (\bm{\mu}_c -
\bar{\mathbf{x}})'
$$
$$
\mathbf{W} = \sum_c \sum_{i \in c}(\mathbf{x}_i - \bm{\mu}_c) (\mathbf{x}_i -
\bm{\mu}_c)'
$$

\smallskip

Because $w$ is invariant to rescaling, we can choose $w$ such that
$w'\mathbf{W}w=1$, leading to:

$$
w^* = \argmax_w w'\mathbf{B}w 
$$
\begin{center}
s.t.
\end{center}
$$
w'\mathbf{W}w=1
$$

This is a generalized eigenvalue problem, with $w$ given by the largest
eigenvalue of $\mathbf{W}^{-1}\mathbf{B}$

\smallskip

If we assume Gaussian class densities and common covariance matrix
$\mathbf{\Sigma}$, then LDA is optimal (equivalent to Bayes).

\medskip

\textbf{Interlude: Lagrangian optimization}

Suppose that we have a standard optimization problem with $m$ inequality and/or
$p$ equality constraints:

\smallskip

Minimize $f_0(x)$

s.t. $f_i(x) \leq 0, \quad i=1...m$

$h_i(x)=0, \quad i = 1...p$

The Lagrangian is:
$$
L(x, \lambda, v) = f_0(x) + \sum_{i=1}^m \lambda_if_i(x) + \sum_{i=1}^p v_i
h_i(x)
$$

The dual function is the minimum value of the Lagrangian over $x$:
$$
g(\lambda, v) = \inf_x L(x, \lambda, v)
$$

This function is concave, even if the original problem is not convex. It also is
always less than or equal to the original objective function evaluated at its
optimal value. Putting these together, if we maximize $\lambda$ and $v$ in the dual, it is the same
as minimizing $x$ in the primal.

\smallskip

Slater's condition: if the primal and dual are feasible, then the gap is zero.

\smallskip

Complementary slackness: if a variable is positive, then the associated dual
constraint must be binding. If a constraint fails to bind, then the associated
variable must be zero.

\medskip

\textbf{Logistic}

Same as usual. Estimate using MLE. Can add a regularizer. Often outperforms LDA
because it doesn't have squared error (bad). Example:

\includegraphics[scale=0.25]{log}

\medskip

\textbf{SVM}

\smallskip

\textit{Hard margin (linearly separable data)}

Suppose we have labels $Y_i \in \{-1,1\}$ and data $X_i$. 

We want to construct a decision hyperplane $\langle w, X \rangle + b= 0$. $w$ is
a weight vector that we apply to $X$. If for a particular observation $\langle w, X_i \rangle + b < 0$, we
classify as $Y_i=-1$. Similarly, if $\langle w, X_i \rangle + b > 0$, we
classify it as $Y_i = 1$

Notation note:
$$b + \langle w, x \rangle = \mathbf{w}^T\mathbf{X} + b = \sum w_ix_i + b$$ 

What makes a good hyperplane? Assuming linear separability, a criterion is that
it maximizes the minimum distance from the data points to the hyperplane. The
distance of a point $x_i$ to a hyperplane $w'x + b$ is:

$$
\frac{|w'x_i + b|}{||w||}
$$

So, we might construct a good hyperplane by trying to maximize the distance to
the closest point:

$$
\max_{w, b} \min\{\frac{|w'x_i + b|}{||w||}\}
$$

We also want to make sure that it classifies correctly, so we add the
below constraint. 

$$
s.t. \quad Y_i(\langle w, x_i \rangle + b) \geq 1 \quad \forall i
$$

A problem, however, is that this won't have a unique solution because we can
scale the problem by some constant $\gamma$ and obtain identical results. We get
around this by stipulating that the hyperplane must be \textit{canonical}
to $X$, which means that the distance between the hyperplane and the closest
point is unity.

$$
\min_{1 \leq i \leq n} |\langle w, x_i \rangle + b| = 1
$$

Since we now know that the distance between the hyperplane and the point closest
to the hyperplane is unity, the problem becomes: 

$$
\max_{w, b}\frac{1}{||w||}
$$
$$
s.t. \min_{1 \leq i \leq n} |\langle w, x_i \rangle + b| = 1
$$
$$
\quad Y_i(\langle w, x_i \rangle + b) \geq 1 \quad \forall i
$$

Maximizing the above function is equivalent to minimizing $||w||$. We will add
some other stuff to the problem to make later work easier.

$$
\min_{w, b}\frac{1}{2}||w||^2
$$
$$
s.t. \min_{1 \leq i \leq n} |\langle w, x_i \rangle + b| = 1
$$
$$
\quad Y_i(\langle w, x_i \rangle + b) \geq 1 \quad \forall i
$$

%\includegraphics[scale=0.25]{svm1}

This is a quadratic function with linear constraints, aka quadratic programming.
The Lagrangian is:
$$
L(b, w, \alpha) = \frac{1}{2}||w||^2 + \sum_{i=1}^n \alpha_i [1 - Y_i(\langle
w, x_i \rangle + b)]
$$

We need the minima to compute the dual. We find them by taking the derivative
and setting equal to 0:

$$
\nabla_w = 
\nabla_w \left( \frac{1}{2}||w||^2 \right) + 
\nabla_w \left(\sum_{i=1}^n \alpha_i \right) - 
\nabla_w \left( \sum_{i=1}^n \alpha_i Y_i \langle w, x_i \rangle \right) - 
\nabla_w \left( \sum_{i=1}^n \alpha_i Y_i b \right)
= 0
$$
$$
w = \sum_{i=1}^n \alpha_i Y_i x_i
$$
$$
\frac{\partial L}{\partial b} = \frac{\partial}{\partial b}
\left( \frac{1}{2}||w||^2 \right) + \frac{\partial}{\partial b}
\left(\sum_{i=1}^n \alpha_i \right) - \frac{\partial}{\partial b}\left( \sum_{i=1}^n \alpha_i Y_i \langle
w, x_i \rangle \right) - \frac{\partial}{\partial b} \left( b \sum_{i=1}^n
\alpha_i Y_i \right)
=0
$$
$$
\sum_{i=1}^n \alpha_i Y_i = 0
$$

We plug them into the Lagrangian to obtain the dual. By
definition, $||v||^2 = \langle v, v \rangle$. So:

$$
\frac{1}{2} ||w||^2 = \frac{1}{2} \langle w, w \rangle = 
\frac{1}{2} \langle \sum_{i=1}^n \alpha_i Y_i x_i, \sum_{j=1}^n \alpha_j Y_j x_j \rangle
$$

The $\alpha$ and $Y$ are just scalars and so we can pull them out:

$$
= \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j Y_i Y_j \langle x_i, x_j \rangle
$$

Now we factor out the right hand side:

$$
\sum_{i=1}^n \alpha_i [1 - Y_i(\langle w, x_i \rangle + b)] = \sum_{i=1}^n
\alpha_i [1 - Y_i \langle w, x_i \rangle - Y_i b]
$$
$$
= \sum_{i=0}^n \alpha_i - \sum_{i=0}^n \alpha_i Y_i \langle w, x_i \rangle -
b \sum_{i=0}^n \alpha_i Y_i
$$

We know that:

$$
b \sum_{i=0}^n \alpha_i Y_i = b \times 0 = 0
$$

Now let's plug $w$ into the middle term:

$$
\sum_{i=0}^n \alpha_i Y_i \langle \sum_{j=0}^n \alpha_j Y_j x_j, x_i \rangle
$$

Again we can pull out the scalars:

$$
= \sum_{i=0}^n \sum_{j=0}^n \alpha_i \alpha_j Y_i Y_j \langle x_i, x_j \rangle
$$

We recombine everything to form the dual:

$$
q(\alpha) = \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j Y_i Y_j
\langle x_i, x_j \rangle + \sum_{i=1}^n \alpha_i - 
\sum_{i=0}^n \sum_{j=0}^n \alpha_i \alpha_j Y_i Y_j \langle x_i, x_j \rangle
$$

$$
= \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=0}^n \sum_{j=0}^n \alpha_i \alpha_j Y_i Y_j \langle x_i, x_j \rangle
$$

Hence, the dual problem is:

$$
\max_{\alpha \in \mathbb{R}^n} 
\sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=0}^n \sum_{j=0}^n \alpha_i \alpha_j Y_i Y_j \langle x_i, x_j \rangle
$$
$$
s.t. \quad \alpha_i \geq 0 \quad \forall i
$$
$$
\quad \sum_{i=0}^n Y_i \alpha_i = 0
$$

The first constraint corresponds to our ``correctness'' constraint in the
objective. The second constraint is a ``balance'' constraint: that the distances
from points to the hyperplane must balance each other out.

Both the primal and dual are feasible, so the dual gap is 0 and solving the dual is
equivalent to solving the primal. 

Looking at this dual gives us some interesting properties. Complementary slackness 
says that if a variable is positive,
then the associated dual constraint must be binding; conversely, if a constraint
fails to bind, then the associated dual variable must be zero. In the primal, we
had the constraint $Y_i(b + \langle w, x_i \rangle)\geq 1$, which is the same as
$1 - Y_i(b + \langle w, x_i \rangle) \leq 0$. If for primal constraint $1 - Y_i(b + \langle w, x_i
\rangle) = 0$, i.e. $x_i$ lays exactly on the margin, then the dual variable
$\alpha_i > 0$. In other words, only points laying on the
margin---\textit{support vectors}---determine the
separating hyperplane. 

This definitely helps our robustness: 
variation of training points won't change our decision hyperplane unless the
points happen to fall on the margin. 

It also can help our computation if we have
some heuristics to determine before computing $w$ what are some likely support
vectors.

In essence, specifying a hyperplane---a function of a simple form $w'x$---helps
us control $d$ and avoid the curse of dimensionality. The support vector
property helps us control $n$.

To determine $b$, we average $Y_i - \langle w, x_i \rangle$ over all $i$ with
$\alpha_i > 0$.

\medskip

\textit{Soft margin}

The above approach may not always be possible (since it requires linear
seperability) or desirable (since it may lead to overfitting). So, we have the
alternative soft margin approach.

We introduce slack variables, as well as a cost parameter to control them. The
problem becomes:

\end{document}



Reminder: 3 methods
LDA
Logistics
SVM

Logistic regression
Bayes classifier is also linear classifier
MLE is equivalent to ERM with respect to linear hypothesis class and logistic
loss
Since logistic loss is convex function of margin y f(x)
Can add a regularizer, typically L1 or L2 - useful to prevent overfitting
If you have more d than n, can always achieve perfectly separate

LDA works poorly bc of squared loss in a classification problem - for squared
loss, points with large margin are penalized

SVM achieves data compression - solution depends ona  small subset of D, the
so-called support vectors

Have to look at both prime and dual
Write down Lagrangian - slide 37
Study SVM this weekend
Hard margin only if linearly separable (and even then might not want to bc of
danger of overfitting)

Soft margin - allow some observations to be incorrectly classified and/or inside
envelope
Bigger the C, the more tolerant you are to violations of envelope
Can show that that formulation equivalent to ERM

Dual allows non-linear decision boundary

When n is large, use primal
When d is large, use dual


